{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ?b!Uw.TZYT\n",
      "OB:y\n",
      "S!qmaTZWjAggXNi-3CWIJci3!BQkeF!lTyDLoQ,agaEI$A$nL;&'tUqfBcx;qZkr!zY:X!v \n",
      "U n'F-h?ATJFpke;w3akkvc$XvlnWi?-C \n",
      "GEAQ P VeRUM\n",
      "GlCIDrilzL&-Stki.gmVolIKkgGRoYBp3Rx WsALbgTsf\n",
      "TXBla,xzv.HO$VMSY \n",
      "----\n",
      "iter 0, loss: 104.359673\n",
      "----\n",
      " san e uosee e U,ze hlogespmhlnm !he AakWy w h tns\n",
      "nescg trets tesnCnennnpngz? elsam s eesrstUpIcS srk r senrtuh lr ndhan Q nsesrehd htctg'tetntni mra hnI,g s oru ehsfth h, p 3ptnl sts s t teseada  t'n \n",
      "----\n",
      "iter 100, loss: 105.696897\n",
      "----\n",
      " evrhfcyAmycy o umghslt\n",
      "H.'e,wees ,\n",
      "e iw,iyade e' brrrk\n",
      "c iel Y ,e st sii,otT oem\n",
      "Ykyrncrrub?ihtOehahilh,ec-u!uiheoie-AeieltiZeue\n",
      "\n",
      "!cppe huoual  ofewfeo,d e,i dy T  aTCrioesi,eoelwyopi,icioP-aT  es\n",
      "mh  \n",
      "----\n",
      "iter 200, loss: 103.810935\n",
      "----\n",
      " ICs.aCowiClqmN kiws\n",
      "m mzR\n",
      "antzt\n",
      "g\n",
      "  stohne,nesFzrh ee tr  dwa gswo A\n",
      " S evqsl ,?awetrutas\n",
      "l\n",
      "etStslthe eetoWOaosCor Atsswe aerma nr ypllomoo r mchCnArheLa ebgre t ehrubon lu 3gncas,mt  yA?Amwovgmesyuef \n",
      "----\n",
      "iter 300, loss: 101.883079\n",
      "----\n",
      " enktpt\n",
      "swtoheihh N,igsG yt hhynbac hhinn hair'a:ln toagteit,aur,nhksleaxou u red hoson fo ssd\n",
      "NO'olaon\n",
      "KFhUnhdO lil ont an d ky '?nshwas mev sstecohoEhds U:preerf,s\n",
      " Tudeei\n",
      "Mostrtskhd ithi'e dooztpepi \n",
      "----\n",
      "iter 400, loss: 99.747499\n",
      "----\n",
      " usomhqgi ?rad onsfre\n",
      "o:egaarreswlrr\n",
      "\n",
      "FmppenspbwiIThi rhdhidb  hinltatathmshaliX!hau uso d acostph:se at hafis's laraublghoreed\n",
      "O\n",
      "Thchlos outhoy\n",
      "\n",
      ":ATtOts r i'lp\n",
      "\n",
      "Oaurharf: afpnel\n",
      "j:\n",
      "Ht mau ,tha au ho i \n",
      "----\n",
      "iter 500, loss: 97.672624\n",
      "----\n",
      " ghies whe nhein tow hh;ene eWr,e\n",
      "aus ids, \n",
      "dDazmouz:\n",
      "ptortimk fenorh edoaussIn wwfYony uilf h ve e,by vvounef orr Kti er enele' e oon sell y,ye nwal.\n",
      "ncsh onedpfaasite eg oredradiee,noir ou  erse at   \n",
      "----\n",
      "iter 600, loss: 95.322571\n",
      "----\n",
      " ,a iuds. og ot is- alkl, ir' iTd  sklildod irf yrord di! n olm\n",
      "MI, scinend ile hot acnlrn e hinywo und hes\n",
      "fzttsming indtpnold iud:Vbeer obedr ofh hoiusl,d, is n; mhapiub hed erp iTlann, sudsm e owin' \n",
      "----\n",
      "iter 700, loss: 93.176197\n",
      "----\n",
      " osar.\n",
      "VAlRLyToit.cybn zol.\n",
      "\n",
      "TQRpvr,d tor Axssta winsI\n",
      "\n",
      "Xor?i\n",
      "AVlCUID\n",
      "Rvpetirreats,rdosor tot u; irullsU\n",
      "SSno wfelnls? jhtaes a\n",
      "$oE?:our Uotonr tatytonoav, gne e yhed Ir acgwge brthlmnev.\n",
      "non. cams a,  \n",
      "----\n",
      "iter 800, loss: 90.926398\n",
      "----\n",
      " \n",
      ", moteel.\n",
      "Hd oul uve a:,amomis \n",
      "sgon anraN\n",
      "\n",
      "veod do, wir pim focrblltone:\n",
      "\n",
      "MIUROMAN:S\n",
      ":Gwsis be' gemsith bes\n",
      "Wegs\n",
      "Teu wgous,h baen fand dho!\n",
      "ARFheme domion\n",
      "S y yayowlary tiaf ee rtheorl.dour ez Se, v \n",
      "----\n",
      "iter 900, loss: 88.778416\n",
      "----\n",
      " d moue pse hse tou 'tpous buke the:onche sow herd igers\n",
      "Tee leceas\n",
      "\n",
      "Towh:\n",
      "Din Taldsrit akFfdet eael me ulur fou  lie ls'il:\n",
      "CYand ur?\n",
      "\n",
      "AEVIAS:\n",
      "?IAWwhhlt ine\n",
      "\n",
      "Hamek od sfrsen gitfis thishll tlat ho owi \n",
      "----\n",
      "iter 1000, loss: 86.822431\n",
      "----\n",
      " I les avate oneand on psels\n",
      "I y? ok.\n",
      "If gin indtome nthholo?, thel'sS\n",
      "The\n",
      "Mf-Is ey yooum Auis\n",
      "MUF\n",
      "imke Akp fvear the Ake lotd htaw hcund A I uanghet Has \n",
      "gEosd sue nouues bomi,\n",
      "A; Pye.\n",
      "\n",
      "Th\n",
      "Tous y aWse \n",
      "----\n",
      "iter 1100, loss: 84.714742\n",
      "----\n",
      " fansooJpaHuns ir terote thesevergyhit-t hthe \n",
      "jees rou theg thd beacshideri sor a\n",
      "US:\n",
      "\n",
      "!hV\n",
      "IUUA:\n",
      "Psouceu, b tod hm wel-mh.\n",
      "IUIN-AOUS:\n",
      ":\n",
      "Th Isless het mrerereatsh ase omanone imalds  ahe f thtis are.\n",
      "\n",
      " \n",
      "----\n",
      "iter 1200, loss: 82.880607\n",
      "----\n",
      " ov mtet toaau\n",
      "Wau onl I-I:\n",
      "Asaads\n",
      "Wwhhhthei ige tiven,r onul lo b, vice goreU:\n",
      "Ukeg MAnNM\n",
      "IAginem, sefe bemtg\n",
      "\n",
      "AS:Wif\n",
      "UYI,WireS:\n",
      "IWif,t thed! Arblos poupel\n",
      "MaboMer\n",
      "I;,Fb saToulm ar pm hoo bobog alu wo \n",
      "----\n",
      "iter 1300, loss: 81.172938\n",
      "----\n",
      " ir teolet\n",
      "\n",
      "An: t an d pot.istoy sarp Ar tos Mne ,d o bes\n",
      "Fus\n",
      "\n",
      "IUIBU:\n",
      "Weetdebrerin! crlit.\n",
      "O\n",
      "COSIPRS\n",
      "LMS MAROUNACIAWgeal!,!jBoond, he hed fele als, tire mans oukatns,\n",
      "e baslat of Vor fund po hs editind \n",
      "----\n",
      "iter 1400, loss: 79.692880\n",
      "----\n",
      " y uer are-encimgancde\n",
      "ze iulers te Ehees ot\n",
      "Bht yhirgo e yae Whe bad mcir an bis,\n",
      "ve csan.\n",
      "We\n",
      "CIu\n",
      "A,\n",
      "File\n",
      "Nlet toeg aE!n\n",
      "JGifl i mos autesgyrif is ias fome thee yunghe wer whes mhsgat hef hban; tof':\n",
      " \n",
      "----\n",
      "iter 1500, loss: 78.181292\n",
      "----\n",
      " on.h htheaasirr inndlathy eintwhia'e;\n",
      "Feytike- ofo a we Vilu\n",
      "gores; chleaan al poln\n",
      "Thecdee,sl;m; he; ms he por th, tous mot tooecomon teu.\n",
      "\n",
      "OSitowir movirle the seplompish yuitwes\n",
      "Worst,\n",
      "cosedin nhdt \n",
      "----\n",
      "iter 1600, loss: 76.822862\n",
      "----\n",
      " as lnert, hor tjore a, \n",
      "ame he wy ths teon,\n",
      "Why aS fabocadt anci'\n",
      "tsh\n",
      "Winre, buntateu oronamome laus, olorod weneyteugtiaca, were os thmirl\n",
      " inr, one om xavoworbee ra.\n",
      "\n",
      "MRMNMUSWrate tif pit o, warvel  \n",
      "----\n",
      "iter 1700, loss: 75.455566\n",
      "----\n",
      " ne reavery'od th uly gome,\n",
      "y an abas morry the the tother\n",
      "sir tt,e wh,nels. pile abte 'y toulvedt fh bid ile thothee e hem herothane zsghe whath yi alsied Ciimrn ci. tout\n",
      "has y,othe nols fouts she who \n",
      "----\n",
      "iter 1800, loss: 74.297703\n",
      "----\n",
      "  phird wean.-\n",
      "Wo rew erlcas toust,, ao heus\n",
      "Tould.wThn ra, ber I errb tim, thorectenlkt soiyl.\n",
      "\n",
      "Andt ene Ct weratd mesyt of apse'l ,y thar hy mas,\n",
      "\n",
      "und\n",
      "Nets ar ind fey\n",
      "Tanen adhis,, blen yny theedsy a \n",
      "----\n",
      "iter 1900, loss: 72.996746\n",
      "----\n",
      " isn dat, c o's otiru st thopztde bhe tot thered\n",
      "Yucad\n",
      ".\n",
      "A Ol,e tkisuriss node ces\n",
      "wo' anc.\n",
      "Fnoree bace orer\n",
      "Thonc torestfou nd\n",
      "Cfor fort'\n",
      "We ere o nome feonw or oun'scama bheg iforl seof ur herd noth  \n",
      "----\n",
      "iter 2000, loss: 71.891218\n",
      "----\n",
      " ube,,, welcneols\n",
      "Soce sof, omcd'y c famte nowavaf pan dyind-: \n",
      "Se aies sepedstore hyet, heal ponasf Whe hsl tme mor\n",
      "sone tays  hakd mofceZr, mosset\n",
      "Hang;\n",
      "Hrele. thit,\n",
      "Wer\n",
      "Serenttouts\n",
      "Th\n",
      "Lohsomu rat. C \n",
      "----\n",
      "iter 2100, loss: 71.009548\n",
      "----\n",
      " s' uroncegCaty  hnsos coue hwiiute poulm fhd,\n",
      "I forek\n",
      "Cil\n",
      "Whilkt:\n",
      "I perits,,\n",
      "Wferer wiinder: wonr andene'n deloe ton mave loue pouk wot\n",
      "Id pegof some tome ghren.\n",
      "not sot tolil uf manj.\n",
      "\n",
      "l3IUS.\n",
      "\n",
      "RIAOZ. \n",
      "----\n",
      "iter 2200, loss: 69.955637\n",
      "----\n",
      " ol ! \n",
      "sasS:\n",
      "Tone ouc.\n",
      "Wher ale meapl, Th tote' g err tot wom byi ei bath\n",
      "y unkonI? stene mem:\n",
      "Wid mmsiuter:\n",
      "Ainkeat?alitouns low taben moultnas vobet shert\n",
      "Ms; arsy co? loy a tour iwb, lhisd aplther s \n",
      "----\n",
      "iter 2300, loss: 69.252579\n",
      "----\n",
      " cemex\n",
      "Neod of.ter s not hes icath tho 'k, wen?\n",
      "\n",
      "have dhout, Ay yo hit ht cor trdath howgi,: morad tin.M\n",
      "Woun.\n",
      "Thost hen aivec\n",
      "rive hi mes them Hiore hpe, the bed:h thed, sow thy!\n",
      "\n",
      "ZAke:\n",
      "I toll krei\n",
      "Ec \n",
      "----\n",
      "iter 2400, loss: 68.345273\n",
      "----\n",
      " ut poy.\n",
      "\n",
      "MAIN::\n",
      "An! bltchatll whos tonu\n",
      "\n",
      "BRIOUIUS:\n",
      "I cer anf gobs.\n",
      "\n",
      "Cakh Nicy,.\n",
      "\n",
      "Sacorstyon I yu cionglt t, inds dof theigt'gor wound non.\n",
      "mnour thand'd us.\n",
      "Wissaucirm Sdarbnyurek yo'nd:\n",
      "UI?\n",
      "Hame whus \n",
      "----\n",
      "iter 2500, loss: 67.553550\n",
      "----\n",
      "  shnug\n",
      " th wimt, iu sake\n",
      "IL\n",
      "OLIAS:\n",
      "Hyat't pis\n",
      "Yous mour benes was: whaceeceunsh toune thed old yRims of.\n",
      "\n",
      "OSNS Iws tee hpd.\n",
      "Ae,\n",
      "Thiis are toce ford urare theres yom ce onaved it beryeu s re hetheoj se \n",
      "----\n",
      "iter 2600, loss: 66.817871\n",
      "----\n",
      " he toue I un mame hnolt.\n",
      "Onu s? hil hdir!\n",
      "Nqop ta pave tpimdd pno'ad beserlay ket?\n",
      "We ertiz3 wandint thess ue bace chtes b, not sire de;\n",
      "iucemgsenofld\n",
      "Yil:\n",
      "Scine.\n",
      "Htin dond 'anr cor e acse''l pol yomf \n",
      "----\n",
      "iter 2700, loss: 66.111409\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d001ed68b1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-668347772c43>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdWhy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdby\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdhnext\u001b[0m \u001b[0;31m# backprop into h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mdhraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m \u001b[0;31m# backprop through tanh nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdbh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checking\n",
    "from random import uniform\n",
    "def gradCheck(inputs, target, hprev):\n",
    "    global Wxh, Whh, Why, bh, by\n",
    "    num_checks, delta = 10, 1e-5\n",
    "    _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)\n",
    "    for param,dparam,name in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], ['Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
    "        s0 = dparam.shape\n",
    "        s1 = param.shape\n",
    "        assert s0 == s1, 'Error dims dont match: %s and %s.' % (repr(s0), repr(s1))\n",
    "        print(name)\n",
    "        for i in range(num_checks):\n",
    "            ri = int(uniform(0,param.size))\n",
    "            # evaluate cost at [x + delta] and [x - delta]\n",
    "            old_val = param.flat[ri]\n",
    "            param.flat[ri] = old_val + delta\n",
    "            cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "            param.flat[ri] = old_val - delta\n",
    "            cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "            param.flat[ri] = old_val # reset old value for this parameter\n",
    "            # fetch both numerical and analytic gradient\n",
    "            grad_analytic = dparam.flat[ri]\n",
    "            grad_numerical = (cg0 - cg1) / ( 2 * delta )\n",
    "            rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "            print('%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))\n",
    "            # rel_error should be on order of 1e-7 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
