{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, LSTM, RepeatVector, TimeDistributed, RNN, Lambda, Concatenate, merge, Flatten\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error, binary_crossentropy\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from Generators import bmove_generator\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import L1L2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def gen(batch_size=128):\n",
    "    while True:\n",
    "        a = np.zeros((batch_size,4)) #x, v, b1, b2\n",
    "        a[:,1] = np.random.uniform(0,0.1,size=batch_size)\n",
    "#         a[:,2] = np.random.uniform(0,-10,size=batch_size)\n",
    "#         a[:,3] = np.random.uniform(0, 10,size=batch_size)\n",
    "        a[:,2] = np.random.uniform(-2,-10,size=batch_size)\n",
    "        a[:,3] = np.random.uniform(2, 10,size=batch_size)\n",
    "        i = np.random.choice([0,1], size=batch_size) #isb1\n",
    "        b = a[:,2]*i + a[:,3]*(1-i)\n",
    "        a[:,1] *= np.sign(1-i-0.5) \n",
    "#         a[:,0] = np.random.uniform(b-3*a[:,1], b-5*a[:,1])\n",
    "        a[:,0] = np.random.uniform(b, b-2*a[:,1])\n",
    "        \n",
    "        isBounce = ( np.abs(a[:,0] + a[:,1]) - np.abs(b)) > 0\n",
    "        \n",
    "        v = a[:,1] * np.sign(1-isBounce-0.5) #new speed\n",
    "        x = (a[:,0] + a[:,1]) * (1-isBounce) + (2*b-a[:,1]-a[:,0])*isBounce\n",
    "        b_newIsb1 = ((x-a[:,2])/(a[:,3]-a[:,2])) < 0.5\n",
    "        b_new = a[:,2]*b_newIsb1 + a[:,3]*(1-b_newIsb1)\n",
    "        y = np.zeros((batch_size,2))\n",
    "        y[:,0] = x\n",
    "        y[:,1] = v\n",
    "\n",
    "        \n",
    "        yield a, y  \n",
    "\n",
    "\n",
    "def gen2(batch_size=128):\n",
    "    while True:\n",
    "        a, y = next(gen(batch_size=batch_size))\n",
    "        a2, y2 = next(gen(batch_size=batch_size))\n",
    "        A, Y = np.concatenate([a,a2],axis=1), np.concatenate([y, y2], axis=1)\n",
    "        yield A, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_load(string):\n",
    "    with open('trained/'+string+'.json', 'r') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights('trained/'+string+'.h5')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 16        \n",
      "=================================================================\n",
      "Total params: 18,196\n",
      "Trainable params: 18,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(8,))\n",
    "\n",
    "h = Dense(128, activation='linear')(inputs)\n",
    "h = Dense(128, activation='relu')(h)\n",
    "h = Dense(4, activation='linear')(h)\n",
    "h = Dense(4, activation='linear', kernel_regularizer=L1L2(l1=0, l2=0), use_bias=False)(h)\n",
    "\n",
    "preds = h\n",
    "\n",
    "model = Model(inputs=inputs, outputs=preds)\n",
    "#TODO: lr decay, no momentum\n",
    "model.compile(loss=mean_squared_error,\n",
    "              optimizer='adam',)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=mean_squared_error,\n",
    "              optimizer=Adam(lr=1e-4))\n",
    "\n",
    "g,f = next(gen2(batch_size=2))\n",
    "\n",
    "model.predict(g)\n",
    "\n",
    "\n",
    "x = model.get_weights()\n",
    "pd.DataFrame(x[0])\n",
    "\n",
    "\n",
    "# In[126]:\n",
    "\n",
    "\n",
    "ws = np.array(model.get_weights())\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "# nw = np.zeros_like(model.get_weights()[0])\n",
    "# nw[0,0] = 1\n",
    "# nw[1,0] = 1\n",
    "# nw[1,1] = 1\n",
    "# nw[4,2] = 1\n",
    "# nw[5,2] = 1\n",
    "# nw[5,3] = 1\n",
    "# model.set_weights([nw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 0.1121\n",
      "Epoch 2/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 0.0023\n",
      "Epoch 3/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 0.0020\n",
      "Epoch 4/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 0.0014\n",
      "Epoch 5/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 9.7975e-04\n",
      "Epoch 6/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 7.6746e-04\n",
      "Epoch 7/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 6.5086e-04\n",
      "Epoch 8/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 6.0078e-04\n",
      "Epoch 9/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.8051e-04\n",
      "Epoch 10/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 5.7420e-04\n",
      "Epoch 11/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 5.6793e-04\n",
      "Epoch 12/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.6522e-04\n",
      "Epoch 13/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 5.6263e-04\n",
      "Epoch 14/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.5823e-04\n",
      "Epoch 15/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.5692e-04\n",
      "Epoch 16/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.5615e-04\n",
      "Epoch 17/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.5176e-04\n",
      "Epoch 18/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.5067e-04\n",
      "Epoch 19/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.4728e-04\n",
      "Epoch 20/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.4508e-04\n",
      "Epoch 21/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.4367e-04\n",
      "Epoch 22/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.4136e-04\n",
      "Epoch 23/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.4127e-04\n",
      "Epoch 24/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.3840e-04\n",
      "Epoch 25/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.3927e-04\n",
      "Epoch 26/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.3420e-04\n",
      "Epoch 27/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.3350e-04\n",
      "Epoch 28/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.3135e-04\n",
      "Epoch 29/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.2679e-04\n",
      "Epoch 30/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.2804e-04\n",
      "Epoch 31/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.2673e-04\n",
      "Epoch 32/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.2237e-04\n",
      "Epoch 33/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.2404e-04\n",
      "Epoch 34/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.2153e-04\n",
      "Epoch 35/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.1756e-04\n",
      "Epoch 36/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.1252e-04\n",
      "Epoch 37/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 5.0553e-04\n",
      "Epoch 38/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.9687e-04\n",
      "Epoch 39/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.8839e-04\n",
      "Epoch 40/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.8231e-04\n",
      "Epoch 41/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.7885e-04\n",
      "Epoch 42/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.7418e-04\n",
      "Epoch 43/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.7119e-04\n",
      "Epoch 44/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.6727e-04\n",
      "Epoch 45/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.6446e-04\n",
      "Epoch 46/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.6207e-04\n",
      "Epoch 47/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.5942e-04\n",
      "Epoch 48/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.5751e-04\n",
      "Epoch 49/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.5518e-04\n",
      "Epoch 50/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.5541e-04\n",
      "Epoch 51/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.5168e-04\n",
      "Epoch 52/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.5030e-04\n",
      "Epoch 53/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.4911e-04\n",
      "Epoch 54/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.4800e-04\n",
      "Epoch 55/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.4821e-04\n",
      "Epoch 56/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.4451e-04\n",
      "Epoch 57/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.4479e-04\n",
      "Epoch 58/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.4255e-04\n",
      "Epoch 59/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.4465e-04\n",
      "Epoch 60/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.4090e-04\n",
      "Epoch 61/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.4061e-04\n",
      "Epoch 62/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.3850e-04\n",
      "Epoch 63/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.3857e-04\n",
      "Epoch 64/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.3560e-04\n",
      "Epoch 65/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.3676e-04\n",
      "Epoch 66/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.3488e-04\n",
      "Epoch 67/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.3458e-04\n",
      "Epoch 68/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.3418e-04\n",
      "Epoch 69/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.3211e-04\n",
      "Epoch 70/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.3048e-04\n",
      "Epoch 71/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.3129e-04\n",
      "Epoch 72/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2970e-04\n",
      "Epoch 73/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2798e-04\n",
      "Epoch 74/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2841e-04\n",
      "Epoch 75/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2787e-04\n",
      "Epoch 76/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2682e-04\n",
      "Epoch 77/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2447e-04\n",
      "Epoch 78/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2487e-04\n",
      "Epoch 79/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2414e-04\n",
      "Epoch 80/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2309e-04\n",
      "Epoch 81/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2275e-04\n",
      "Epoch 82/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2133e-04\n",
      "Epoch 83/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2076e-04\n",
      "Epoch 84/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.2039e-04\n",
      "Epoch 85/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.1845e-04\n",
      "Epoch 86/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.1816e-04\n",
      "Epoch 87/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.1824e-04\n",
      "Epoch 88/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.1774e-04\n",
      "Epoch 89/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.1564e-04\n",
      "Epoch 90/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1594e-04\n",
      "Epoch 91/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1571e-04\n",
      "Epoch 92/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1356e-04\n",
      "Epoch 93/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1463e-04\n",
      "Epoch 94/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1337e-04\n",
      "Epoch 95/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1428e-04\n",
      "Epoch 96/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1270e-04\n",
      "Epoch 97/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1193e-04\n",
      "Epoch 98/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1138e-04\n",
      "Epoch 99/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1070e-04\n",
      "Epoch 100/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.1005e-04\n",
      "Epoch 101/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.0922e-04\n",
      "Epoch 102/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.0912e-04\n",
      "Epoch 103/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.0842e-04\n",
      "Epoch 104/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.0855e-04\n",
      "Epoch 105/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.0770e-04\n",
      "Epoch 106/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.0760e-04\n",
      "Epoch 107/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.0557e-04\n",
      "Epoch 108/600\n",
      "12800/12800 [==============================] - 16s 1ms/step - loss: 4.0566e-04\n",
      "Epoch 109/600\n",
      "12800/12800 [==============================] - 17s 1ms/step - loss: 4.0525e-04\n",
      "Epoch 110/600\n",
      " 3209/12800 [======>.......................] - ETA: 12s - loss: 4.0447e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d97272b3ebc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "model.fit_generator(gen2(), steps_per_epoch=12800, epochs=600)\n",
    "\n",
    "X, Y = next(gen2(batch_size=1280))\n",
    "P = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_model(model, model_str):\n",
    "#     model_json = model.to_json()\n",
    "#     with open('trained/'+model_str+\".json\", \"w\") as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     # serialize weights to HDF5\n",
    "#     model.save_weights('trained/'+model_str+\".h5\")\n",
    "#     print(\"Saved model to disk\")\n",
    "# save_model(model, 'model_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa3b03315f8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = next(gen(batch_size=1))\n",
    "X2, Y2 = next(gen(batch_size=1))\n",
    "# P = module.predict(a)\n",
    "\n",
    "b1, b2 = X[0,2:]\n",
    "c1, c2 = X2[0,2:]\n",
    "\n",
    "A = np.concatenate([X,X2], axis=-1)\n",
    "\n",
    "preds = []\n",
    "newA = A\n",
    "preds += [A[0,[0,1,4,5]]]\n",
    "for _ in range(10):\n",
    "    P = model.predict(newA)\n",
    "    preds += [P[0]]\n",
    "    x, xv, y, yv = P[0]\n",
    "    newA = np.array([[x,xv, b1, b2, y, yv, c1, c2]])\n",
    "preds = np.array(preds)\n",
    "\n",
    "m1,n1 = preds[0,[0,2]]\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set(xlim=(m1-0.2, m1+0.2), ylim=(n1-0.2,n1+0.2))\n",
    "line = ax.plot([0,1],[0,0])\n",
    "\n",
    "x,xv,y,yv = preds[0]\n",
    "# plt.quiver(x, y, xv, yv)\n",
    "plt.scatter(preds[:,0], preds[:,2])\n",
    "x,xv,y,yv = preds[0]\n",
    "plt.quiver(x, y, xv, yv)\n",
    "plt.plot([b1,b1+0.0001],[c1-5,c2])\n",
    "\n",
    "plt.plot([b1,b1],[c1,c2])\n",
    "plt.plot([b1,b2],[c1,c1])\n",
    "plt.plot([b1,b2],[c2,c2])\n",
    "plt.plot([b2,b2],[c1,c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# model.set_weights(ogweights)\n",
    "\n",
    "\n",
    "X, Y = next(gen(batch_size=1000))\n",
    "X2, Y2 = next(gen(batch_size=1000))\n",
    "A = np.concatenate([X,X2], axis=-1)\n",
    "P = model.predict(A)\n",
    "B = np.concatenate([Y,Y2], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-3811f0df6f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preds.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 mgr = init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 424\u001b[0;31m                                    copy=copy)\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ai/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mprep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must pass 2-d input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input"
     ]
    }
   ],
   "source": [
    "delta = deepcopy(model.get_weights())\n",
    "\n",
    "theta = []\n",
    "for weights in delta:\n",
    "    theta += [np.random.normal(scale=0.5, size=weights.shape) * weights]\n",
    "    \n",
    "def get_weight_combo(alpha):\n",
    "    l = []\n",
    "        \n",
    "    for i in range(len(delta)):\n",
    "        l += [ delta[i]*alpha + theta[i]*(1-alpha) ]\n",
    "    return l\n",
    "\n",
    "\n",
    "rmse_list = []\n",
    "alphas = np.linspace(0, 2, num=101)\n",
    "for alpha in alphas:\n",
    "    weights = get_weight_combo(alpha)\n",
    "    model.set_weights(weights)\n",
    "    P = model.predict(A)\n",
    "    rmse_list += [mse(B, P)**0.5]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set(xlim=(0, 2))\n",
    "# line = ax.plot([0,1],[0,0])\n",
    "\n",
    "plt.plot(alphas, rmse_list)\n",
    "\n",
    "\n",
    "# In[81]:\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "# In[94]:\n",
    "\n",
    "\n",
    "# fig.savefig('lossdense.png')\n",
    "\n",
    "\n",
    "h = Dense(8, activation='relu')(inputs)\n",
    "h = Dense(8, activation='relu')(h)\n",
    "h = Dense(8, activation='relu')(h)\n",
    "h = Dense(4)(h)\n",
    "\n",
    "preds = h\n",
    "\n",
    "model = Model(inputs=inputs, outputs=preds)\n",
    "model.compile(loss=mean_squared_error,\n",
    "              optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(gen2(), steps_per_epoch=12800, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
